{"cells":[{"cell_type":"markdown","metadata":{"id":"WnbkuvxW0Ntp"},"source":["# Prostate segmentation evaluation on IDC collection -- ProstateX\n","*   Dataset : [ProstateX]()\n","*   Goal : Prostate segmentation using Task24 Prostate nnU-net pre-trained model, T2 input"]},{"cell_type":"markdown","source":["resample preds to idc image data with .nrrd == LPS orientation instead of RAS == necessary for DICOM conversion of AI segs"],"metadata":{"id":"I7JJ4plOh9di"}},{"cell_type":"markdown","source":["# Custom functions"],"metadata":{"id":"TZALQu_dnyFq"}},{"cell_type":"code","source":["def parse_json_dcmqi(json_path):\n","  out_dic = {}\n","  data = json.load(open(json_path))\n","  for segment_dic in data[\"segmentAttributes\"][0]:\n","    # print(segment_dic.keys())\n","    out_dic[segment_dic[\"SegmentedPropertyTypeCodeSequence\"][\"CodeMeaning\"]] \\\n","    = segment_dic[\"labelID\"]\n","  return out_dic"],"metadata":{"id":"t3n_sUaUmMHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_image_dcm_to_nrrd(input_path, output_path_root, target_format=\"nrrd\", prefix=\"\"):\n","  if not os.path.exists(output_path_root): \n","    !mkdir -p $output_path_root\n","  !dcm2niix -z y -m y -f %i_{prefix} -o $output_path_root $input_path\n","  # out_path_file = f\"{output_path_root}/{prefix}.nrrd\" \n","  # !plastimatch convert \\\n","  # --input $input_path \\\n","  # --output-img $out_path_file"],"metadata":{"id":"ADnWwCO22Dpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_seg_to_nii(input_path, output_path):\n","  if not os.path.exists(output_path): \n","    !mkdir -p $output_path\n","  \n","  print(f'input path : {input_path}')\n","  print(f'output_path : {output_path}')\n","  !segimage2itkimage --inputDICOM $input_path --outputDirectory $output_path \\\n","  --outputType nii "],"metadata":{"id":"Pabz_xHTFzxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_dcm_sorted(input_path,output_path, idc_df):\n","  for serie_folder in sorted(glob.glob(os.path.join(input_path, \"**\", \"**\", \"*\"))):#, recursive = True):\n","    path_serie_dcm_lst = glob.glob(os.path.join(serie_folder, \"*.dcm\"))\n","    modality = idc_df[idc_df[\"SeriesInstanceUID\"] == path_serie_dcm_lst[0].split('/')[-2]][\"Modality\"].iloc[0]#'SEG' if pydicom.dcmread(path_serie_dcm_lst[0]).Modality == \"SEG\" else \"MR\"\n","    seriesInstanceUID = serie_folder.split(\"/\")[-1]\n","    studyInstanceUID = serie_folder.split(\"/\")[-2]\n","    patientID = serie_folder.split(\"/\")[-3]\n","    print(f\"Serie processed : {serie_folder}\")\n","    print(f\"SeriesDescription : {pydicom.read_file(glob.glob(os.path.join(serie_folder, '*.dcm'))[0]).SeriesDescription}\")\n","    print(f\"Modality : {pydicom.read_file(glob.glob(os.path.join(serie_folder, '*.dcm'))[0]).Modality}\")\n","    #convert to nii\n","    convert_image_dcm_to_nrrd(input_path=serie_folder, \n","                           output_path_root=output_path,\n","                           prefix=f\"{seriesInstanceUID}\")"],"metadata":{"id":"HO2dWXPBF3Sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://pydicom.github.io/pydicom/stable/tutorials/dicom_json.html\n","def get_seg_dcm_tags_pydicom(seg_path_dcm):\n","  ds = pydicom.dcmread(seg_path_dcm)\n","  # print(ds)\n","  dcm_dict = ds.to_json_dict()\n","  out_dict = {\n","  'ReferencedSeriesInstanceUID' : dcm_dict['00081115']['Value'][0]['0020000E']['Value'][0], #RefSerieUID == correspond to T2,\n","  'StudyInstanceUID' : dcm_dict['0020000D']['Value'][0],\n","  'patientID' : dcm_dict['00100020']['Value'][0],# patientID \n","  'SOPClassUID' : dcm_dict['00080016']['Value'][0], # SOP Class UID\n","  'SOPInstanceUID' : dcm_dict['00080018']['Value'][0],\n","  'SeriesInstanceUID' : dcm_dict['0020000E']['Value'][0],\n","  'Modality' : dcm_dict['00080060']['Value'][0], # Modality \n","  'SeriesDescription' : dcm_dict['0008103E']['Value'][0], # SeriesDescription\n","  'studydesc' : dcm_dict['00081030']['Value'][0], # StudyDescription\n","  'series_time' : dcm_dict['00080031']['Value'][0],#SeriesTime\n","  'study_time' : dcm_dict['00080030']['Value'][0],#StudyTime\n","  'series_date' : dcm_dict['00080021']['Value'][0], #SeriesDate\n","  'study_date' : dcm_dict['00080020']['Value'][0] #StudyDate\n","  }\n","  # 00081115 == Referenced Series Sequence\n","  # 0020000E == Referenced Series Instance UID\n","  return out_dict"],"metadata":{"id":"6FMfDpLRnzv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_ohif_url_nnunet(row, datastore='', dataset='', app=''):\n","    #test\n","    app='fir-idc-prostate-ohif.web.app'\n","    project='idc-sandbox-003'\n","    location='us-central1'\n","    dataset='prostate-seg'\n","    datastore='whole_prostate_nnunet_id24_2ServersIDC' #whole_prostate_nnunet_id24_2ServersIDC #pz_tz_nnunet_id05_2ServersIDC\n","    studyUID = row['StudyInstanceUID']\n","    return f'https://{app}/viewer/{studyUID}!secondGoogleServer=/projects/{project}/locations/{location}/datasets/{dataset}/dicomStores/{datastore}'\n","    #\"https://fir-idc-prostate-ohif.web.app/projects/idc-sandbox-003/locations/us-central1/datasets/prostate-seg/dicomStores/prostatex_no_gt_datastore/study/\"+studyUID"],"metadata":{"id":"EThUsFJNoqo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_idc_data_serie_uid_seg(idc_df, out_path):\n","  # save the list of GCS URLs into a file\n","  selection_manifest = os.path.join(os.environ[\"IDC_Downloads\"], \"idc_manifest.txt\")\n","  idc_df[\"gcs_url\"].to_csv(selection_manifest, header=False, index=False)\n","  # let's make sure the download folder is clean, in case you ran this cell earlier\n","  # for a different dataset\n","  # !rm -rf {os.environ[\"IDC_Downloads\"]+\"/*.dcm\"}\n","  !cat {selection_manifest} | gsutil -m cp -I {os.environ[\"IDC_Downloads\"]}\n","  !python dicomsort/dicomsort.py -k -u {os.environ[\"IDC_Downloads\"]} {os.environ[\"IDC_Downloads_Sorted\"]}/%PatientID/%StudyInstanceUID/%SeriesInstanceUID/%SOPInstanceUID.dcm\n","  # !rm -rf {os.environ[\"qin_prostate_rep_dicom\"]+\"/*\"} \n","  in_mv = os.environ['IDC_Downloads_Sorted']+'/*'\n","  !mv $in_mv $out_path\n","  #{os.environ[\"qin_prostate_rep_dicom\"]}\n","  # convert_dcm_sorted(input_path=os.environ[\"qin_prostate_rep_dicom\"],\n","  #                 output_path=os.environ[\"qin_prostate_rep_root\"], idc_df=selection_df) "],"metadata":{"id":"Fj5E_8qrGcjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_idc_data_serie_uid(idc_df, out_path, out_path_nii):\n","  # save the list of GCS URLs into a file\n","  selection_manifest = os.path.join(os.environ[\"IDC_IMG_Downloads\"], \"idc_manifest.txt\")\n","  idc_df[\"gcs_url\"].to_csv(selection_manifest, header=False, index=False)\n","  # let's make sure the download folder is clean, in case you ran this cell earlier\n","  # for a different dataset\n","  # !rm -rf {os.environ[\"IDC_Downloads\"]+\"/*.dcm\"}\n","  !cat {selection_manifest} | gsutil -m cp -I {os.environ[\"IDC_IMG_Downloads\"]}\n","  !python dicomsort/dicomsort.py -k -u {os.environ[\"IDC_IMG_Downloads\"]} {os.environ[\"IDC_IMG_Downloads_Sorted\"]}/%PatientID/%StudyInstanceUID/%SeriesInstanceUID/%SOPInstanceUID.dcm\n","  # !rm -rf {os.environ[\"qin_prostate_rep_dicom\"]+\"/*\"} \n","  in_mv = os.environ['IDC_IMG_Downloads_Sorted']+'/*'\n","  !mv $in_mv $out_path\n","  #{os.environ[\"qin_prostate_rep_dicom\"]}\n","  convert_dcm_sorted(input_path=out_path,\n","                  output_path=out_path_nii, idc_df=idc_df) "],"metadata":{"id":"9zBhg_TO0P_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reformat_image_nnunet():\n","  #reformats images to correct format, \n","  #from global path to nnunet folder==nnUNet preprocessed\n","  for mr_vol in glob.glob(os.path.join(os.environ[\"qin_prostate_rep_nii\"], f\"*.nii.gz\")):\n","    serieUID = mr_vol.split('/')[-1].split(\"_\")[1].replace(\".nii.gz\",\"\")#.split(\".\")[0]\n","    patientID = mr_vol.split('/')[-1].split(\"_\")[0]\n","    nnunet_idx = \"0000\" #if \"T2\" in mr_vol.split('/')[-2] else \"0001\"#0000 for T2 and 0001 for ADC\n","    nnunet_path = os.path.join(os.environ[\"nnUNet_preprocessed\"], \n","                                \"_\".join([patientID, serieUID, nnunet_idx]) + \".nii.gz\") \n","    !cp $mr_vol $nnunet_path"],"metadata":{"id":"nfxSP-K6GpFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def largest_component_retrieval(input_path : str, output_path=None):\n","    \"\"\"Largest component retrieval \n","    Args:\n","        input_path (str): input seg nifti path, binary image\n","        output_path (str): output_path after conversion\n","    Convert binary image into a connected component image, each component has an integer label.\n","    Relabel components so that they are sorted according to size (there is an optional minimumObjectSize parameter to get rid of small components).\n","    Get largest connected componet, label==1 in sorted component image.\n","    \"\"\"\n","    assert os.path.exists(input_path)\n","    input_image = sitk.ReadImage(input_path, imageIO=\"NiftiImageIO\")\n","    assert len(np.unique(sitk.GetArrayFromImage(input_image))) == 2 # make sure its a binary image\n","    component_image = sitk.ConnectedComponent(input_image)\n","    sorted_component_image = sitk.RelabelComponent(component_image, sortByObjectSize=True)\n","    largest_component_binary_image = sorted_component_image == 1\n","    if output_path is not None: \n","        # assert os.path.exists(output_path)\n","        print(output_path)\n","        sitk.WriteImage(largest_component_binary_image, output_path, imageIO=\"NiftiImageIO\")\n","    else:\n","        print(\"No writing on disk of largest component of input.\")\n","    # sanity checks == logs\n","    print(f\"Input path : {input_path}\")\n","    print(f\"Output path : {output_path}\")\n","    print(f\"Number of components found : {len(np.unique(sitk.GetArrayFromImage(sorted_component_image)))}\")\n","    # print(\"Done!\")\n","    # print(\"\\n\")"],"metadata":{"id":"Z_Xrcq4nHOa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resample_preds(input_path_nnunet_preds=\"\", input_path_t2_idc=\"\", output_path=\"\"):\n","  for pred_path in sorted(glob.glob(os.path.join(input_path_nnunet_preds, \"*.nii.gz\"))):\n","    search_t2_path = os.path.join(input_path_t2_idc, \\\n","                                  f\"{pred_path.split('/')[-1].split('_')[0]}_{pred_path.split('/')[-1].split('_')[1]}*.nii.gz\") #PatientID_SerieUID.nii.gz\n","                                      #get serieUID\n","    print(f\"search path for : {search_t2_path}\")\n","    t2_path = glob.glob(search_t2_path, recursive=True)[0]                              \n","    print(f\"path found : {t2_path}\")\n","    print(f\"pred path : {pred_path}\")\n","    resample_args_to_t2_origin = {\"input\" : pred_path, \n","                          \"output\" : os.path.join(output_path, \n","                                                  f\"{pred_path.split('/')[-1][:-7]}_resampled.nii.gz\"),\n","                          \"fixed\" : t2_path,\n","                          \"interpolation\" : \"nn\"}\n","    \n","    path_log = os.path.join(os.environ[\"logs\"], 'log_pypla_res_pred' + pred_path.split('/')[-1].split('.')[0] + '.txt')            \n","    !touch $path_log\n","    pypla.resample(verbose = False, **resample_args_to_t2_origin, path_to_log_file=path_log)\n","    print()"],"metadata":{"id":"LE6hjzqkIiAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resample_idc_data(input_path=\"\", input_path_t2_idc=\"\", output_path=None):\n","  for idc_seg_path in sorted(glob.glob(os.path.join(input_path, \"*.nii.gz\"))):\n","    search_t2_path = os.path.join(input_path_t2_idc, \"**\", \"*.nii.gz\") #PatientID_SerieUID.nii.gz\n","                                      #get serieUID\n","    print(f\"search path for : {search_t2_path}\")\n","    t2_path = glob.glob(search_t2_path, recursive=True)[0]                              \n","    print(f\"path found : {t2_path}\")\n","    print(f\"idc_seg_path : {idc_seg_path}\")\n","    if output_path is None : \n","      output_path=idc_seg_path\n","    resample_args_to_t2_origin = {\"input\" : idc_seg_path, \n","                          \"output\" : output_path,\n","                          \"fixed\" : t2_path,\n","                          \"interpolation\" : \"nn\"}\n","    pypla.resample(verbose = False, **resample_args_to_t2_origin)\n","    print()"],"metadata":{"id":"7FbyOj3e1oML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seg_nii_to_dicom(idc_df, input_path_nii=\"\", input_path_dcm_idc=\"\", output_path_root=\"\"):\n","  assert os.path.exists(input_path_nii)\n","  assert os.path.exists(input_path_dcm_idc)\n","  !mkdir -p $output_path_root\n","  for nii_seg_pred in glob.glob(os.path.join(input_path_nii, '*.nii.gz')):\n","    patID = nii_seg_pred.split('/')[-1].split('_')[0]\n","    study_mr_t2_serieUID = nii_seg_pred.split('/')[-1].split('_')[1].replace(\".nii.gz\",\"\")\n","\n","    # study_mr_t2_serieUID = idc_df[idc_df[\"StudyInstanceUID\"] == study_current][\"SeriesInstanceUID\"].unique()[0]\n","    #find t2 dcm folder\n","    t2_dcm_folder = glob.glob(os.path.join(input_path_dcm_idc, patID, \"**\", study_mr_t2_serieUID))[0]\n","    #find seg dcm file\n","    # find nii seg folder == preds resampled\n","    assert os.path.exists(t2_dcm_folder)\n","    print('\\nConverting...')\n","    print(f'pred nnunet procssed : {nii_seg_pred}')\n","    print(f't2_dcm_folder : {t2_dcm_folder}')\n","    output_path = os.path.join(output_path_root, '_'.join([patID, study_mr_t2_serieUID])+'.dcm')\n","    #find gt seg dcm file == orginal idc dcm files\n","    #convert nii pred to dcm\n","    !itkimage2segimage --inputImageList $nii_seg_pred \\\n","    --inputDICOMDirectory  $t2_dcm_folder \\\n","    --inputMetadata  $seg_dcm_metadata_json_file \\\n","    --outputDICOM $output_path \n","    print(\"Done!\")"],"metadata":{"id":"7RS5TpGSGoe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_dice(pred_path, gt_path):\n","  #computation of dice score\n","  out_plast = !plastimatch dice --dice $gt_path $pred_path\n","  dice_score = [el[1] for el in out_plast.fields() if \"DICE\" in el[0]][0]#formatting by plastimatch dice output, retrieve of DSC\n","  return float(dice_score)"],"metadata":{"id":"KDV_5w-1hYzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_hsdff(pred_path, gt_path):\n","  #computation of dice score\n","  out_plast = !plastimatch dice --hausdorff $gt_path $pred_path\n","  hsdff = [el[5] for el in out_plast.fields() if \"Percent\" in el[0] and \"distance\" in el[3]][0]#formatting by plastimatch hsdff output, retrieve of hsdff\n","  return float(hsdff)"],"metadata":{"id":"OVyNQP9Cb2tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_hsdff_regular(pred_path, gt_path):\n","  #computation of dice score\n","  out_plast = !plastimatch dice --hausdorff $gt_path $pred_path\n","  hsdff = [el[3] for el in out_plast.fields() if \"Hausdorff\" in el[0] and \"distance\" in el[1]][0]#formatting by plastimatch hsdff output, retrieve of hsdff\n","  return float(hsdff)"],"metadata":{"id":"OSy98b7zmxaB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_avg_surface_dist(pred_path, gt_path):\n","  return asd(result=sitk.GetArrayFromImage(sitk.ReadImage(pred_path)), \\\n","             reference=sitk.GetArrayFromImage(sitk.ReadImage(gt_path)))"],"metadata":{"id":"NtmROiLxPb7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Example output by plastimatch dice --hausdorff\n","# Hausdorff distance = 7.419876\n","# Avg average Hausdorff distance = 0.457145\n","# Max average Hausdorff distance = 0.535726\n","# Percent (0.95) Hausdorff distance = 3.117000\n","# Hausdorff distance (boundary) = 7.419876\n","# Avg average Hausdorff distance (boundary) = 1.015289\n","# Max average Hausdorff distance (boundary) = 1.158696\n","# Percent (0.95) Hausdorff distance (boundary) = 3.818646"],"metadata":{"id":"ASB4VkdUiiE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8GPDXZW00j5"},"source":["# Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSg6W2HXzsoG"},"outputs":[],"source":["#colab \n","from google.colab import auth\n","auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"y1ZYYVPHz_xm"},"source":["# Setup GCP Project ID"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1EN9Dnz0CU4"},"outputs":[],"source":["import os\n","project_id = \"idc-sandbox-003\"\n","os.environ[\"GCP_PROJECT_ID\"] = project_id"]},{"cell_type":"markdown","metadata":{"id":"eRtmCas7dT6D"},"source":["# Setup of the Colab VM\n","\n","\n","\n","In the following cells we will confirm you have a GPU before doing anything else, and will install and import all the Python dependencies. \n","\n","The main python packages we need to install are:\n","* `nnunet` - which is the [codebase for the nn-UNet framework](https://github.com/MIC-DKFZ/nnUNet) we are going to be using for the segmentation step;\n","* `pydicom`, a Python [package](https://github.com/pydicom/pydicom) that lets the use read, modify, and write DICOM data in an easy \"pythonic\" way - that we are going to use to distinguish different DICOM objects from each other."]},{"cell_type":"markdown","metadata":{"id":"zLvysANUArnm"},"source":["## GPU checks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pf2j172mddvK"},"outputs":[],"source":["# check wether the Colab Instance was correctly initialized with a GPU instance\n","gpu_list = !nvidia-smi --list-gpus\n","\n","has_gpu = False if \"failed\" in gpu_list[0] else True\n","\n","if not has_gpu:\n","  print(\"Your Colab VM does not have a GPU - check \\\"Runtime > Change runtime type\\\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kL6xCvo3eKQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829125709,"user_tz":420,"elapsed":5,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"f64a23de-1240-4019-be67-cb9e934388bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 11 18:18:44 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# check which model of GPU the notebook is equipped with - a Tesla K80 or T4\n","# T4 is the best performing on the two - and can about half the GPU processing time\n","\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"ZJhwBz2ABT_p"},"source":["## Environment Setup\n","\n","Here we will configure the Linux environment variables needed to run the nnU-Net pipeline. \n","\n","Three main variables are needed by default to run the nnU-Net segmentation pipelines:\n","* `nnUNet_raw_data_base` is the path to the folder where the segmentation pipeline expects to find the data to process;\n","* `nnUNet_preprocessed` is the path to the folder where the preprocessed data are saved;\n","* `RESULTS_FOLDER` is the path to the folder storing by default the model weights and, in our case, for simplicity, the segmentation masks produced by the pipeline.\n","\n","We will use the additional variable `PATH_TO_MODEL_FILE` to point to the location where the pre-trained model weights for the chosen model will be stored (more on this later).\n","\n","Please notice that these variables need to be set using `os.environ[]` in Google Colab - as `!export` is not sufficient to guarantee the variables are kept from one cell to the other. For more in-depth information regarding what the nnU-Net framework uses these folders for, please visit [the dedicated nnU-Net documentation page](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md)"]},{"cell_type":"markdown","metadata":{"id":"40E3HnI5A0SX"},"source":["## Install command-line tools\n","\n","\n","[Plastimatch](https://plastimatch.org/index.html) is considered to be the swiss army knife of medical images processing: we will use it to convert DICOM (CT, RTSTRUCT) series to NRRD files - but it can be used for a multitude of other tasks, such as registration, resampling, cropping, and computing statistics to name a few. Plastimatch is also available as a 3DSlicer plug-in and can be used directly from the Slicer GUI.\n","\n","For the sake of clarity and simplicity, we will call Plastimatch from a very simple [Python wrapper](https://github.com/denbonte/pyplastimatch) written for the occasion (unfortunately, Plastimatch does not provide an official one)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZL6-ByHA7XY"},"outputs":[],"source":["%%capture\n","!sudo apt update\n","\n","!sudo apt install plastimatch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMovHnKgBEfC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829157687,"user_tz":420,"elapsed":1606,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"954adc1e-297c-4ddf-c920-bcd8e7f299bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["plastimatch version 1.8.0\n"]}],"source":["!echo $(plastimatch --version)"]},{"cell_type":"markdown","metadata":{"id":"K_8zlhmdo0HD"},"source":["[dcmqi](https://github.com/QIICR/dcmqi) is an open source library that can help with the conversion between imaging research formats and the standard DICOM representation for image analysis results. More specifically, you can use dcmqi convert DICOM Segmentation objects (DICOM SEG) into research formats, such as NIfTI and NRRD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAkmz4jApGh8"},"outputs":[],"source":["%%capture\n","!wget https://github.com/QIICR/dcmqi/releases/download/v1.2.5/dcmqi-1.2.5-linux.tar.gz\n","!tar zxvf dcmqi-1.2.5-linux.tar.gz\n","!cp dcmqi-1.2.5-linux/bin/* /usr/local/bin/"]},{"cell_type":"markdown","metadata":{"id":"fFRWendbBH72"},"source":["Finally, we are going to install [Subversion](https://subversion.apache.org/), a tool that will allow us to clone GitHub repositories only partially (to save time and space)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBEJRe-2BKah"},"outputs":[],"source":["%%capture\n","\n","!sudo apt install subversion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI1_BixWBN1O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829165981,"user_tz":420,"elapsed":17,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"ad7cd9b3-cbb9-420a-c645-397f3ae43c71"},"outputs":[{"output_type":"stream","name":"stdout","text":["svn, version 1.13.0 (r1867053) compiled May 12 2022, 20:47:08 on x86_64-pc-linux-gnu\n"]}],"source":["!echo $(svn --version | head -n 2)"]},{"cell_type":"markdown","metadata":{"id":"0Ev-JlpCAuMs"},"source":["## Install Python packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XMzq-8nRvKd"},"outputs":[],"source":["%%capture\n","!pip install nnunet\n","!pip install pydicom\n","!pip install nibabel\n","!pip install dcm2niix\n","!pip install SimpleITK\n","!pip install medpy"]},{"cell_type":"markdown","metadata":{"id":"62yhEkkjITzn"},"source":["Unpack and install model we downloaded earlier (under `PATH_TO_MODEL_FILE`). This step can take about 1-2 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOVsPuzxIOXX"},"outputs":[],"source":["%%capture\n","!nnUNet_install_pretrained_model_from_zip $PATH_TO_MODEL_FILE"]},{"cell_type":"markdown","metadata":{"id":"BP1tahM9tzlm"},"source":["Next we set up few things to help with visualization of the segmentations later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cURjj8rzAa2L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829239644,"user_tz":420,"elapsed":1386,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"e2729207-8db2-4d9e-c473-c7ca3e96d2c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","This Colab instance is equipped with a GPU.\n"]}],"source":["import os\n","import sys\n","import shutil\n","import csv\n","import random\n","\n","import os\n","import glob\n","import csv\n","import json\n","\n","import nibabel as nib\n","\n","import time\n","import gdown\n","\n","import json\n","import pprint\n","import numpy as np\n","import pandas as pd\n","\n","import pydicom\n","import nibabel as nib\n","import SimpleITK as sitk\n","from medpy.metric.binary import asd\n","# from medpy.metric.binary import dc as dice_coef\n","# from medpy.metric.binary import hd as hausdorff_distance\n","# from medpy.metric.binary import asd as avg_surf_distance\n","\n","# from medpy.filter.binary import largest_connected_component\n","\n","# use the \"tensorflow_version\" magic to make sure TF 1.x is imported\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras\n","\n","print(\"\\nThis Colab instance is equipped with a GPU.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrAh2kjqb2Be","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829242847,"user_tz":420,"elapsed":3206,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"fa7c0b7e-0197-459c-fea9-1c2f26ceb0bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["A    pyplastimatch/__init__.py\n","A    pyplastimatch/pyplastimatch.py\n","A    pyplastimatch/utils\n","A    pyplastimatch/utils/__init__.py\n","A    pyplastimatch/utils/data.py\n","A    pyplastimatch/utils/eval.py\n","A    pyplastimatch/utils/viz.py\n","Checked out revision 25.\n"]}],"source":["# PyPlastimatch - python wrapper for Plastimatch (and interactive notebook visualisation)\n","!svn checkout https://github.com/AIM-Harvard/pyplastimatch/trunk/pyplastimatch pyplastimatch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsePTNBmM9sw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683829243793,"user_tz":420,"elapsed":952,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"c16c4c31-c42c-4542-d7b7-28103dc8023b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'dicomsort'...\n","remote: Enumerating objects: 169, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (26/26), done.\u001b[K\n","remote: Total 169 (delta 23), reused 34 (delta 17), pack-reused 126\u001b[K\n","Receiving objects: 100% (169/169), 87.85 KiB | 1.13 MiB/s, done.\n","Resolving deltas: 100% (86/86), done.\n"]}],"source":["# dicomsort is the pythong package that can sort DICOM files into\n","# folder organization based on user-specified DICOM attributes\n","!git clone https://github.com/pieper/dicomsort.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0s9aRassGOQ_"},"outputs":[],"source":["from pyplastimatch import pyplastimatch as pypla\n","from pyplastimatch.utils import viz as viz_utils\n","from pyplastimatch.utils import data as data_utils"]},{"cell_type":"markdown","metadata":{"id":"Duil1tviGWaG"},"source":["# Data selection, downloading and structuring -- Conversion to DICOM\n","\n","We want to select here the collection named qin-prostate repeatibility, and more particularly the two timepoints per patient ID for further analysis."]},{"cell_type":"markdown","metadata":{"id":"naJ5OAKZnZpG"},"source":["In order to use data hosted by IDC effectively, you will need to utilize metadata to navigate what data is available and to select specific files that are relevant in your analysis. The main metadata table you will need for this purpose is the [`bigquery-public-data.idc_current.dicom_all`](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=idc_current&t=dicom_all&page=table) table.\n","\n","This query has one row per file hosted by IDC. All of IDC data is in DICOM format, and each of the rows in this table will have all of the DICOM attributes extracted from a given file. It will also have various columns containing non-DICOM metadata, such as the name of the collection where the file is included, size of the file, and URL that can be used to retrieve that file.\n","\n","To query IDC BigQuery tables, you can use one of the following approaches:\n","1. `%%bigquery` magic will allow you to define your query in plain SQL, and load the result of the query into a Pandas dataframe.\n","2. [BigQuery Python API](https://googleapis.dev/python/bigquery/latest/index.html) is more flexible in allowing you to parameterize your query.\n","3. [Google Cloud BigQuery console](https://console.cloud.google.com/bigquery) is very convenient for interactive query exploration of tables.\n","4. [`gcloud bq`](https://cloud.google.com/bigquery/docs/bq-command-line-tool) is the command line tool that comes as part of [Cloud SDK](https://cloud.google.com/sdk) and is convenient for scripting interactions from the shell. Cloud SDK is preinstalled on Colab.\n","\n","In the following cells we will utilize `%%bigquery`, Python BigQuery SDK and BigQuery console for working with IDC BigQuery tables.\n","\n","First, to verify that you are authenticated, and your project ID is working, let's run a test query against IDC BigQuery table to get the summary statistics about the  of data available in IDC.\n"]},{"cell_type":"markdown","metadata":{"id":"t9YKMGVvVZX_"},"source":["Given `SeriesInstanceUID` value identifying the image series, we can query the IDC metadata table to get the list of files (defined by the Google Storage URLs) corresponding to this series.\n","\n","All of the DICOM metadata for each of the DICOM files is available in the BigQuery table we will be querying. We will get not just the `gcs_url`, but also identifiers for the Study, Series and Instance, to better understand organization of data, and since `StudyInstanceUID` will be handy later when we get to the visualization of the data."]},{"cell_type":"code","source":["from google.cloud import bigquery\n","bq_client = bigquery.Client(os.environ[\"GCP_PROJECT_ID\"])"],"metadata":{"id":"uLzAa7kH4mBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get SerieUIDs in bucket processed from nnunet"],"metadata":{"id":"QjirvyGv_poC"}},{"cell_type":"code","source":["!rm bucketUIDs_nnunet.csv\n","!gcloud storage ls --recursive gs://idc_prostatex/model1/preds_processed_dcm/* > bucketUIDs_nnunet.csv\n","serieUID_nnunet_processed = pd.read_csv(\"bucketUIDs_nnunet.csv\", names=[\"serieUID\"], skiprows=[0])\n","seriesInstanceUID_nnunet_processed_lst = [x.split(\"/\")[-1].split(\"_\")[1].replace(\".dcm\",\"\") for x in serieUID_nnunet_processed.serieUID.values]"],"metadata":{"id":"-rSpFb0qAICu","executionInfo":{"status":"ok","timestamp":1683829254683,"user_tz":420,"elapsed":10032,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b156999-275c-420a-9d59-6d847ea65bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'bucketUIDs_nnunet.csv': No such file or directory\n"]}]},{"cell_type":"code","source":["len(seriesInstanceUID_nnunet_processed_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClvXN2pIPfag","executionInfo":{"status":"ok","timestamp":1683829254683,"user_tz":420,"elapsed":15,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"0a79d111-7896-47d8-80fa-3d372dbab1e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["66"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["Get SerieUIDs from dicom store metadata"],"metadata":{"id":"YOAsoXVmPRmy"}},{"cell_type":"code","source":["from google.cloud import bigquery\n","selection_query = f\"\"\"\n","SELECT\n","  DISTINCT(ReferencedSeriesSequence[SAFE_OFFSET(0)].SeriesInstanceUID) as RefSerieUID\n","FROM\n","  `idc-sandbox-003.prostatex_serieUID.model1_dcm_metadata`\"\"\" \n","selection_result = bq_client.query(selection_query)\n","selection_df = selection_result.result().to_dataframe()\n","dcm_refSerieUID = selection_df.RefSerieUID.values"],"metadata":{"id":"ZzNvW3i7PRm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dcm_refSerieUID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UejPeVdJQp0Z","executionInfo":{"status":"ok","timestamp":1683829257574,"user_tz":420,"elapsed":3,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"318c8802-b858-4dca-f49c-abdcf8bc36f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["66"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["assert sorted(seriesInstanceUID_nnunet_processed_lst) == sorted(dcm_refSerieUID)"],"metadata":{"id":"44GU-MZpRB8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.cloud import bigquery\n","selection_query = f\"\"\"\n","    SELECT \n","      DISTINCT(RefSerieUID)\n","    FROM \n","      `idc-sandbox-003.prostatex_serieUID.model1_preds_eval` as eval_table\"\"\" \n","selection_result = bq_client.query(selection_query)\n","selection_df = selection_result.result().to_dataframe()\n","eval_refSerieUID = selection_df.RefSerieUID.values"],"metadata":{"id":"xdts6TK6lMhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["not_processed_RefSerieUID = set(seriesInstanceUID_nnunet_processed_lst) - set(eval_refSerieUID)"],"metadata":{"id":"gZPHV5dNld9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(not_processed_RefSerieUID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDmyTitNlxNE","executionInfo":{"status":"ok","timestamp":1683829259463,"user_tz":420,"elapsed":5,"user":{"displayName":"Cosmin Ciausu","userId":"01060908221005396239"}},"outputId":"7a5c63ba-95f4-4bde-90e4-1bed15cd577c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["# Main loop"],"metadata":{"id":"kNuuz_84JqnK"}},{"cell_type":"code","source":["from google.cloud import bigquery\n","selection_query = f\"\"\"\n","    SELECT \n","      *\n","    FROM \n","      `bigquery-public-data.idc_v14.dicom_all` as dc_all\n","    WHERE \n","      collection_id='prostatex'\n","    AND \n","      Modality = 'MR' \n","    AND SeriesInstanceUID IN UNNEST(%s)\"\"\" %seriesInstanceUID_nnunet_processed_lst\n","selection_result = bq_client.query(selection_query)\n","selection_t2_df = selection_result.result().to_dataframe()"],"metadata":{"id":"zjqgHoMLO_-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.cloud import bigquery\n","selection_query = f\"\"\"\n","  SELECT\n","    *,\n","    ReferencedSeriesSequence[SAFE_OFFSET(0)].SeriesInstanceUID as RefSerieUID\n","  FROM\n","    `bigquery-public-data.idc_v14.dicom_all`\n","  WHERE\n","    collection_id = 'prostatex'\n","    AND SegmentSequence[SAFE_OFFSET(0)].SegmentedPropertyTypeCodeSequence[SAFE_OFFSET(0)].CodeMeaning = 'Prostate'\n","    AND SegmentSequence[SAFE_OFFSET(0)].SegmentedPropertyTypeCodeSequence[SAFE_OFFSET(0)].CodeValue = '41216001'\n","    AND ReferencedSeriesSequence[SAFE_OFFSET(0)].SeriesInstanceUID IN UNNEST(%s)\n","    \"\"\" %seriesInstanceUID_nnunet_processed_lst\n","selection_result = bq_client.query(selection_query)\n","selection_t2_seg_prostate_df = selection_result.result().to_dataframe()"],"metadata":{"id":"dzoYmMjPgYgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os \n","#nnunet_preds\n","os.environ[\"nnunet_preds\"] = os.path.join(os.getcwd(), \"nnunet_preds\")\n","os.environ[\"nnunet_preds_dcm\"] = os.path.join(os.environ[\"nnunet_preds\"], \"dcm\")\n","os.environ[\"nnunet_preds_nii\"] = os.path.join(os.environ[\"nnunet_preds\"], \"nii\")\n","#seg files\n","os.environ[\"idc_seg\"] = os.path.join(os.getcwd(), \"idc_seg\")\n","os.environ[\"idc_seg_dcm\"] = os.path.join(os.environ[\"idc_seg\"], \"dcm\")\n","os.environ[\"idc_seg_nii\"] = os.path.join(os.environ[\"idc_seg\"], \"nii\")\n","#idc image files\n","os.environ[\"idc_data\"] = os.path.join(os.getcwd(), \"idc_data\")\n","os.environ[\"idc_data_dcm\"] = os.path.join(os.environ[\"idc_data\"], \"dcm\")\n","os.environ[\"idc_data_nii\"] = os.path.join(os.environ[\"idc_data\"], \"nii\")\n","#idc data\n","os.environ[\"IDC_Downloads\"] = os.path.join(os.getcwd(), \"IDC_Downloads\")\n","os.environ[\"IDC_Downloads_Sorted\"] = os.path.join(os.getcwd(), \"IDC_Downloads_Sorted\")\n","os.environ[\"IDC_IMG_Downloads\"] = os.path.join(os.getcwd(), \"IDC_IMG_Downloads\")\n","os.environ[\"IDC_IMG_Downloads_Sorted\"] = os.path.join(os.getcwd(), \"IDC_IMG_Downloads_Sorted\")\n","#evaluation\n","os.environ[\"prostatex_analysis\"] = os.path.join(os.getcwd(), \"prostatex_analysis\")\n","os.environ[\"prostatex_analysis_results\"] = os.path.join(os.environ[\"prostatex_analysis\"], \"results\")\n","os.environ[\"prostatex_analysis_verbose\"] = os.path.join(os.environ[\"prostatex_analysis\"], \"results_verbose\")\n","#radiomics"],"metadata":{"id":"Tm_wILGfRU7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reset_folders():\n","  for key, path in os.environ.items():\n","    check_patterns = [True for el in [\"nnunet_preds\", \"idc\", \n","                                      \"IDC\", \"prostatex_analysis\"] if el in key]\n","    if True in check_patterns:\n","      !rm -rf $path\n","      !mkdir -p $path"],"metadata":{"id":"EtaG_IfLJ3SA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#whole process\n","for serieUID_current in list(seriesInstanceUID_nnunet_processed_lst)[10:11]:#not_processed_RefSerieUID\n","  #reset processing folders\n","  reset_folders()\n","  #download nnunet data\n","  !gsutil -m cp -r gs://idc_prostatex/model1/preds_processed_dcm/*{serieUID_current}* {os.environ['nnunet_preds_dcm']}/\n","  ##convert to nii\n","  convert_seg_to_nii(input_path=glob.glob(f\"{os.environ['nnunet_preds_dcm']}/*.dcm\")[0], \\\n","                    output_path=os.environ['nnunet_preds_nii'])\n","  #download_idc_data\n","  idc_data_df = selection_t2_df[selection_t2_df.SeriesInstanceUID \\\n","                                                           == serieUID_current]\n","  download_idc_data_serie_uid(idc_df=idc_data_df,\n","                              out_path=os.environ[\"idc_data_dcm\"],\n","                              out_path_nii=os.environ[\"idc_data_nii\"])\n","  #download idc seg data\n","  idc_seg_data_df = selection_t2_seg_prostate_df[selection_t2_seg_prostate_df.RefSerieUID \\\n","                                                           == serieUID_current]\n","  download_idc_data_serie_uid_seg(idc_df=idc_seg_data_df,\n","                                       out_path=os.environ[\"idc_seg_dcm\"])\n","  assert len(np.unique(idc_data_df.SeriesInstanceUID.values)) == 1\n","  assert idc_data_df.SeriesInstanceUID.values[0] == idc_seg_data_df.RefSerieUID.values[0]\n","  assert len(np.unique(idc_seg_data_df.PatientID.values)) == 1\n","  assert len(np.unique(idc_seg_data_df.StudyInstanceUID.values)) == 1\n","  assert len(np.unique(idc_seg_data_df.SeriesInstanceUID.values)) == 1\n","  ##convert idc segs to nii\n","  for seg_dcm in glob.glob(f\"{os.environ['idc_seg_dcm']}/**/*.dcm\", recursive=True):\n","    convert_seg_to_nii(input_path=seg_dcm, output_path=os.environ['idc_seg_nii']) \n","  ##resample idc_segs to corresponding image serie\n","  resample_idc_data(input_path=os.environ['idc_seg_nii'],\n","                    input_path_t2_idc=os.environ[\"idc_data_nii\"])\n","  seg_dic = parse_json_dcmqi(glob.glob(os.path.join(os.environ[\"idc_seg_nii\"], \\\n","                                        \"**\", \"*.json\"), recursive=True)[0])\n","  nnunet_dic = parse_json_dcmqi(glob.glob(os.path.join(os.environ[\"nnunet_preds_nii\"], \\\n","                                        \"**\", \"*.json\"), recursive=True)[0])\n","  print(f\"nnunet segment and labelIDs : {nnunet_dic}\")\n","  print(f\"idc seg segment and labelIDs : {seg_dic}\")\n","  for key, value in nnunet_dic.items():\n","    pred_path = glob.glob(os.path.join(os.environ[\"nnunet_preds_nii\"],\\\n","                                       f\"{value}.nii.gz\"))[0]\n","    idc_seg_path = glob.glob(os.path.join(os.environ[\"idc_seg_nii\"],\\\n","                                       f\"{seg_dic[key]}.nii.gz\"))[0]\n","    assert sitk.ReadImage(pred_path).GetSize() == sitk.ReadImage(idc_seg_path).GetSize()\n","    # assert [round(x,2) for x in sitk.ReadImage(pred_path).GetDirection()]\\\n","    #  == [round(x,2) for x in sitk.ReadImage(idc_seg_path).GetDirection()]\n","    # #evaluation\n","    rows_to_insert = [\n","        {\"RefSerieUID\": serieUID_current, \"dice_score\": compute_dice(pred_path, idc_seg_path), \\\n","          \"hsdff\" : compute_hsdff_regular(pred_path, idc_seg_path), \\\n","          \"hsdff_95\" : compute_hsdff(pred_path, idc_seg_path), \\\n","          \"asd\" : compute_avg_surface_dist(pred_path, idc_seg_path),\\\n","          \"regions\" : key}]\n","    print(rows_to_insert)\n","    # upload to bigquery table\n","    # Construct a BigQuery client object.\n","    client = bigquery.Client()\n","    table_id = \"idc-sandbox-003.prostatex_serieUID.model1_preds_eval\"\n","    errors = client.insert_rows_json(table_id, rows_to_insert)  # Make an API request.\n","    if errors == []:\n","        print(\"New rows have been added.\")\n","    else:\n","        print(\"Encountered errors while inserting rows: {}\".format(errors))"],"metadata":{"id":"_OzTkdEZEJZS"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1_VtyrjdfRqe2waLI2cAsUj-g264yEzFv","timestamp":1683783630247},{"file_id":"14g1Vs5LlcbXGmCMG4tIN1rNVTVokYgYf","timestamp":1681923240531},{"file_id":"1zzbZ5cLeStImabaXSKR06azFpTIbnRox","timestamp":1681764723771},{"file_id":"14NRwvoWZlZNI2DbSI_T6hkowkJRDMOso","timestamp":1681406236695},{"file_id":"1UcDxYTvGhXFL1ZUDNkis3yJUln7PIA60","timestamp":1667231814832},{"file_id":"1LR11ePaeZdpsBDEH-hu59Rv85JoM-TAF","timestamp":1664223887456},{"file_id":"https://github.com/ccosmin97/IDC-Prostate_segmentation/blob/main/IDC_notebooks/idc-prostate_segmentation_prostate05_qin-rep-repeat.ipynb","timestamp":1661986443154}],"collapsed_sections":["zLvysANUArnm","40E3HnI5A0SX"],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}